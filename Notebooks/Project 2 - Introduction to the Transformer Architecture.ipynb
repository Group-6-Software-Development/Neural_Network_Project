{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Introduction to the Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tehtävä 2: Luku 11.4: Tekstin luokittelu Transformer encoderin avulla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1us/step\n",
      "WARNING:tensorflow:From c:\\Users\\roope\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.7018 - loss: 0.5349 - val_accuracy: 0.8852 - val_loss: 0.2799\n",
      "Epoch 2/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9280 - loss: 0.1903 - val_accuracy: 0.8830 - val_loss: 0.2921\n",
      "Epoch 3/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9646 - loss: 0.1041 - val_accuracy: 0.8734 - val_loss: 0.3876\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8534 - loss: 0.4538\n",
      "Test Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, LayerNormalization, Dense, Dropout, MultiHeadAttention, GlobalAveragePooling1D, Layer\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Data Loading\n",
    "max_features = 20000\n",
    "sequence_length = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=sequence_length)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=sequence_length)\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):  # Set default value for training\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2   # Number of attention heads\n",
    "ff_dim = 32     # Hidden layer size in feed-forward network\n",
    "\n",
    "# Model\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x, training=True)  # Add training argument explicitly\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile and Train\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=3, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tehtävä 2: Luku 11.5: Kielenkääntäjä Transformerin avulla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asenna tässä kohtaa konsolissa: pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tätä 11.5 kohtaa en saa toimii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shapes:\n",
      "Encoder Inputs: (64, 40)\n",
      "Decoder Inputs: (64, 40)\n",
      "Target Outputs: (64, 40)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_inputs', 'decoder_inputs']. Received: the structure of inputs={'encoder_inputs': '*', 'decoder_inputs': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1885 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000002014B244400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1885 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000002014B244400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 926ms/step - accuracy: 0.8033 - loss: 1.4985 - val_accuracy: 0.8370 - val_loss: 0.7475\n",
      "Epoch 2/3\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 904ms/step - accuracy: 0.8266 - loss: 0.7859 - val_accuracy: 0.8340 - val_loss: 0.6976\n",
      "Epoch 3/3\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 960ms/step - accuracy: 0.8279 - loss: 0.6944 - val_accuracy: 0.8326 - val_loss: 0.6741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x200df7af050>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, LayerNormalization, Dense, Dropout, MultiHeadAttention, Input, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Data Loading: English-Portuguese translation dataset\n",
    "dataset_name = \"ted_hrlr_translate/pt_to_en\"  # You can replace with any seq2seq dataset\n",
    "examples, metadata = tfds.load(dataset_name, with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "# Preprocessing\n",
    "max_tokens = 20000\n",
    "sequence_length = 40\n",
    "\n",
    "# Tokenizer setup\n",
    "def tokenize_pairs(pt, en):\n",
    "    return vectorize_layer(pt), vectorize_layer(en)\n",
    "\n",
    "vectorize_layer = TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "train_text = train_examples.map(lambda pt, en: pt)  # Tokenize Portuguese only\n",
    "vectorize_layer.adapt(train_text.batch(64))\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Transformer Decoder Layer\n",
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.att1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.att2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "\n",
    "    def call(self, enc_output, target, training):\n",
    "        attn1 = self.att1(target, target)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(target + attn1)\n",
    "        attn2 = self.att2(out1, enc_output)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "# Model Building\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "\n",
    "encoder_inputs = Input(shape=(sequence_length,), name=\"encoder_inputs\")\n",
    "x = Embedding(max_tokens, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, num_heads, ff_dim)(x, training=True)\n",
    "\n",
    "decoder_inputs = Input(shape=(sequence_length,), name=\"decoder_inputs\")\n",
    "y = Embedding(max_tokens, embed_dim)(decoder_inputs)\n",
    "decoder_outputs = TransformerDecoder(embed_dim, num_heads, ff_dim)(encoder_outputs, y, training=True)\n",
    "\n",
    "# Adjusting output shape and final layer\n",
    "decoder_outputs = ReshapeLayer((-1, sequence_length, embed_dim))(decoder_outputs)\n",
    "outputs = Dense(max_tokens, activation=\"softmax\")(decoder_outputs)\n",
    "transformer = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "\n",
    "# Compile Model\n",
    "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Function to vectorize and prepare batches\n",
    "def prepare_batch(pt, en):\n",
    "    # Vectorize inputs\n",
    "    pt_vectorized = vectorize_layer(pt)  # Shape: (sequence_length,)\n",
    "    en_vectorized = vectorize_layer(en)  # Shape: (sequence_length,)\n",
    "\n",
    "    # Ensure proper shape (batch_size, sequence_length)\n",
    "    pt_vectorized = tf.ensure_shape(pt_vectorized, [None])  # Ensure it remains rank 1\n",
    "    en_vectorized = tf.ensure_shape(en_vectorized, [None])\n",
    "\n",
    "    # Pad the target sequence for decoder inputs\n",
    "    en_vectorized = tf.pad(en_vectorized, [[0, 1]], constant_values=0)  # Pad with 1 zero\n",
    "\n",
    "    # Return dictionary with encoder and decoder inputs\n",
    "    return {\n",
    "        \"encoder_inputs\": pt_vectorized,            # Encoder inputs\n",
    "        \"decoder_inputs\": en_vectorized[:-1]       # Decoder inputs (without last token)\n",
    "    }, en_vectorized[1:]                          # Target sequence (without first token)\n",
    "\n",
    "# Set up the dataset pipeline\n",
    "train_dataset = train_examples.map(prepare_batch).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_examples.map(prepare_batch).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Confirm the shape of batches\n",
    "for batch in train_dataset.take(1):\n",
    "    print(\"Batch Shapes:\")\n",
    "    print(\"Encoder Inputs:\", batch[0][\"encoder_inputs\"].shape)\n",
    "    print(\"Decoder Inputs:\", batch[0][\"decoder_inputs\"].shape)\n",
    "    print(\"Target Outputs:\", batch[1].shape)\n",
    "\n",
    "# Define the TextVectorization layer with fixed sequence length\n",
    "VOCAB_SIZE = 10000  # Example vocab size\n",
    "MAX_SEQUENCE_LENGTH = 40  # Example max length\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Check if input and output shapes match\n",
    "transformer.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tehtävä 2: Luku 12.1: Generatiivinen kielimalli Transformer-arkkitehtuurilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 9.9330\n",
      "Epoch 2/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0612 - loss: 9.6824\n",
      "Epoch 3/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.4898 - loss: 9.4542\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Generated text:\n",
      " To be or not to be, that is the questionlookeroncravensmacks<UNK>wonderdstartleswintersgnat<UNK><UNK><UNK>several<UNK>pushedlikesbondage<UNK><UNK>steeldfireeyedpikeslimber<UNK><UNK><UNK>affrightsstones<UNK>fever<UNK>prophesiedfabricheareth<UNK>crossedministerdmarianaheardsthaver<UNK><UNK><UNK>disposedstranglingdelayinterred<UNK>mannd<UNK>horrible\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dense, Dropout, Input, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "max_tokens = 20000\n",
    "sequence_length = 50\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Dataset: Simple example with TensorFlow's Shakespeare dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Tokenizer: Splitting text into sentences for better adaptation\n",
    "text_split = text.split('\\n')  # Split by lines or sentences\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=sequence_length)\n",
    "vectorize_layer.adapt(text_split)\n",
    "\n",
    "# Prepare dataset\n",
    "sequences = vectorize_layer([text])[0]\n",
    "inputs = sequences[:-1]\n",
    "targets = sequences[1:]\n",
    "inputs = tf.expand_dims(inputs, axis=-1)  # Ensure inputs have shape (None, sequence_length)\n",
    "targets = tf.expand_dims(targets, axis=-1)  # Ensure targets have shape (None, sequence_length)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Transformer Components\n",
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.att1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn1 = self.att1(inputs, inputs)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn1)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Model Building\n",
    "decoder_inputs = Input(shape=(sequence_length,), name=\"input_layer_0\")  # Define decoder input layer\n",
    "embedding_layer = Embedding(input_dim=max_tokens, output_dim=embed_dim)\n",
    "x = embedding_layer(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, num_heads, ff_dim, dropout_rate)(x, training=True)  # Ensure training=True is passed\n",
    "outputs = Dense(max_tokens, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=decoder_inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "model.fit(dataset, epochs=3)\n",
    "\n",
    "# Text Generation\n",
    "def sample_next(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, num_tokens, temperature=1.0):\n",
    "    # Vectorize seed_text and reshape it to (1, sequence_length)\n",
    "    input_text = vectorize_layer([seed_text])  # Shape: (1, sequence_length)\n",
    "    input_text = input_text[0]  # Remove extra batch dimension\n",
    "    generated_text = seed_text\n",
    "    vocab = vectorize_layer.get_vocabulary()  # Ensure the vocabulary is fetched correctly\n",
    "    \n",
    "    for _ in range(num_tokens):\n",
    "        # Ensure input_text has the shape (1, sequence_length) for model prediction\n",
    "        predictions = model.predict(tf.expand_dims(input_text, axis=0))[0, -1]  # Shape: (max_tokens,)\n",
    "        next_index = sample_next(predictions, temperature)\n",
    "        \n",
    "        # Make sure we are within bounds of vocabulary\n",
    "        if next_index < len(vocab):\n",
    "            next_word = vocab[next_index]\n",
    "        else:\n",
    "            next_word = \"<UNK>\"  # Handle out-of-vocabulary index (in case of model mistakes)\n",
    "        \n",
    "        generated_text += next_word\n",
    "        # Update the input for the next prediction (shifting the window)\n",
    "        input_text = np.append(input_text[1:], [next_index])\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example of text generation\n",
    "seed_text = \"To be or not to be, that is the question\"\n",
    "generated_text = generate_text(seed_text, num_tokens=50, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "While the output is not perfect, it shows that the model is learning and generating text based on its training. The <UNK> tokens are a result of limitations in the vocabulary or training data, and with more epochs and adjustments, the model should improve in generating more sensible text.\n",
    "\n",
    "If you're aiming for better quality, consider the improvements mentioned above, especially in terms of training time, data, and vocabulary size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
